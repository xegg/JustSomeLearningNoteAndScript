I've seen some course material from MIT, and it was shockingly bad. They had teaching materials which required VC5, bunches of implicit global variables, passing colours as "Blue" instead of 32bit ARGB, let alone 4x [0,1] floats, that sort of thing. I wouldn't trust a curriculum or code just because it comes from a big-name university.

My CS degree (from a university which is top 10 in the UK for CS) consisted of:

First year:

OOP- the super basics
Computer Systems- stuff like, binary integer representations.
Basic relational database theory
Mathematics for CS- simple 2D and 3D geometry.
A little bit of HTML/JS- complete beginner's stuff
An equally tiny bit of PHP.
A tad of functional programming
Second year:

Legal issues in computing- stuff like, laws revolving around protection of user data
Programming languages- Chomsky hierarchy and lexing was covered
Operating Systems, Networks, and the Internet- mostly stuff like virtual memory and paging, IP stack
2D computer graphics- mostly just proving theorems of the underlying mathematics
AI- basic descriptions of neural networks, Bayesian belief systems, etc.
Requirements analysis- brief overview of UML, functional/nonfunctional requirements.
Team project
Third year:

Algorithm analysis- complexity theory, mostly
Implementation of programming languages- LL/LR parsing techniques, CFGs, and such things.
Software Project Management- a look at Waterfall/Agile models
International Computing- Unicode and other localization fun
Advanced AI- don't know, honestly, and I've got an exam on it soon
3D computer graphics- mostly, again, just proving theorems for rotation matrices and such
Agent-based Systems- mostly about asynchronous agents communicating, reaching group decisions, etc.
Microprocessor Applications- digital signal processing
Robotics- covers stuff like computer vision and robot decision making at a high level
As you'll notice, pretty much everything is "the basics" of something and almost nothing is covered to a useful depth.

The stuff that was actually worth doing, essential:

OOP- and then some more, and then some more
Functional programming- also some more. Try to pick a language like C++ or C# where you don't have to re-learn the syntax and tools, etc, to cover both styles.
The OS part- virtual memory is good to know about, as is kernel mode vs user mode. Skip segmentation and the IP stack.
Requirements analysis- Gotta be useful for any project
Algorithm analysis- knowing what algorithmic complexity is, how to reduce it, and what the complexity is of common operations is important.
Software project management models- many shops do Agile and many older ones still do Waterfall-style models.
International computing- Unicode is essential
The stuff that was worth doing, optionally:

Programming languages- Chomsky hierarchy, the tools of lexing and parsing. Skip the theory behind LL or LR parsers- an LR parser can accept virtually any realistic unambiguous CFG, and when it can't, your parser generator's documentation will tell you about it.
3D Graphics. I don't mean "Prove this is a rotation matrix formula" wastes of time, I mean actual "This is a vertex shader" stuff, or GPGPU. That's fun, interesting, and different.
Some of the AI stuff is fun- like potential fields and pathfinding.
Stuff that's essential but I didn't cover it anyway:

Concurrency- a must-know, at least the basics, for anyone in 2012.
The rest were a complete waste of time. Unfortunately, most of these nine points I either already knew, or picked up the useful parts elsewhere. If you read about things like the FizzBuzz problem it rapidly becomes apparent that you don't actually need to know all that much to be on top of the pack- which is fortunate, since my degree and many of the materials I've seen online for other degrees really do not teach much at all.

